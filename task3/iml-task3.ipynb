{"cells":[{"cell_type":"code","execution_count":184,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:53.174467Z","iopub.status.busy":"2023-05-09T20:21:53.173692Z","iopub.status.idle":"2023-05-09T20:21:58.544866Z","shell.execute_reply":"2023-05-09T20:21:58.543562Z","shell.execute_reply.started":"2023-05-09T20:21:53.174406Z"},"trusted":true},"outputs":[],"source":["# This serves as a template which will guide you through the implementation of this task.  It is advised\n","# to first read the whole template and get a sense of the overall structure of the code before trying to fill in any of the TODO gaps\n","# First, we import necessary libraries:\n","from sklearn import preprocessing\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, TensorDataset\n","import os\n","import numpy as np\n","import torch\n","from torchvision import transforms\n","import torchvision.datasets as datasets\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torchvision.io import read_image\n","from torchvision.models import resnet50, ResNet50_Weights"]},{"cell_type":"code","execution_count":185,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.547897Z","iopub.status.busy":"2023-05-09T20:21:58.547264Z","iopub.status.idle":"2023-05-09T20:21:58.554827Z","shell.execute_reply":"2023-05-09T20:21:58.553521Z","shell.execute_reply.started":"2023-05-09T20:21:58.547860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1.22.3\n"]}],"source":["print(np.__version__)"]},{"cell_type":"code","execution_count":186,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.556995Z","iopub.status.busy":"2023-05-09T20:21:58.556478Z","iopub.status.idle":"2023-05-09T20:21:58.581493Z","shell.execute_reply":"2023-05-09T20:21:58.580118Z","shell.execute_reply.started":"2023-05-09T20:21:58.556951Z"},"trusted":true},"outputs":[],"source":["embedding_size_global = 2048\n","input_scaler = 0\n","torch.manual_seed(3473)\n","torch.cuda.manual_seed_all(3473)\n","np.random.seed(3473)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","# not tested yet. should be able to run geneeate embeddings (if there are not bugs, which there probably are)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Generate embeddings"]},{"cell_type":"code","execution_count":187,"metadata":{},"outputs":[],"source":["def generate_embeddings():\n","    \"\"\"\n","    Transform, resize and normalize the images and then use a pretrained model to extract \n","    the embeddings.\n","    \"\"\"\n","    # TODO: define a transform to pre-process the images\n","    print(\"-- genearte embeddings --\")\n","\n","    # turns an image into a PyTorch tensor (matrix where each entry is a vector (e.g., for rgb information))\n","    # according to PyTorch (see https://pytorch.org/vision/stable/models.html), \"All the necessary information \n","    # for the inference transforms of each pre-trained model is provided on its weights documentation\"\n","    # This is because only if the input data matches with the format of the data that was used for training\n","    # do we have a certain quality guarantee of the output (here the embeddings) \n","    # need to check at https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n","    # how to correctly preprocess, right now, we are just using transforms.ToTensor\n","    train_transforms = transforms.Compose([transforms.ToTensor()])\n","\n","    train_dataset = datasets.ImageFolder(root=\"dataset/\", transform=train_transforms)\n","    train_dataset_imgs = datasets.ImageFolder(root=\"dataset/\")\n","\n","    #train_dataset = datasets.ImageFolder(root=\"dataset/\")\n","\n","    print(\"-- loaded data set --\")\n","\n","    # Hint: adjust batch_size and num_workers to your PC configuration, so that you don't \n","    # run out of memory\n","    train_loader = DataLoader(dataset=train_dataset,\n","                              batch_size=64,\n","                              shuffle=False,\n","                              pin_memory=True, num_workers=16)\n","\n","    # TODO: define a model for extraction of the embeddings (Hint: load a pretrained model,\n","    #  more info here: https://pytorch.org/vision/stable/models.html)\n","    # model = nn.Module()\n","    embeddings = []\n","    # embedding_size = 1000 # Dummy variable, replace with the actual embedding size once you \n","\n","   # Useing pretrained model ResNet for now\n","\n","    print(\"-- modifiying pretrained model --\")\n","\n","    # here we define the pre-trained model obtained from PyTorch\n","    weights = ResNet50_Weights.DEFAULT\n","    model = resnet50(weights=weights)\n","\n","    # Use the model to extract the embeddings. Hint: remove the last layers of the \n","    # model to access the embeddings the model generates. \n","    newmodel = torch.nn.Sequential(*(list(model.children())[:-1]))\n","    newmodel.eval() #prepares the new model for evaluation (not all models need this)\n","\n","\n","    embedding_size = embedding_size_global # Dummy variable, replace with the actual embedding size once you \n","    # pick your model\n","    num_images = len(train_dataset)\n","    embeddings = np.zeros((num_images, embedding_size))\n","    # TODO: Use the model to extract the embeddings. Hint: remove the last layers of the \n","    # model to access the embeddings the model generates. \n","    preprocess = weights.transforms()\n","\n","    print(\"-- preprocessing --\")\n","\n","    print(train_loader)\n","\n","    #this is the slow way of doing it\n","    for i,(a,b) in enumerate(train_dataset): \n","        if(i % 100 == 0): print(i)\n","        batch = preprocess(a).unsqueeze(0)\n","        result = newmodel(batch).detach()\n","        # transform each\n","        embeddings[i] = result.numpy().reshape((embedding_size)) \n","   \n","\n","    # for [a,b] in train_loader:\n","    #     print(a,b)\n","\n","    # for i, x in enumerate(train_loader):\n","    #     if(i % 1000): print(i)\n","\n","    #     batch = preprocess(a).unsqueeze(0)\n","    #     result = newmodel(batch).detach()\n","    #     embeddings[i*train_loader.batch_size+j] = result.numpy().reshape((embedding_size)) \n","\n","    np.save('dataset/embeddings.npy', embeddings)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load the training and test data"]},{"cell_type":"code","execution_count":188,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.583984Z","iopub.status.busy":"2023-05-09T20:21:58.583504Z","iopub.status.idle":"2023-05-09T20:21:58.598324Z","shell.execute_reply":"2023-05-09T20:21:58.596669Z","shell.execute_reply.started":"2023-05-09T20:21:58.583945Z"},"trusted":true},"outputs":[],"source":["def get_data(file, train=True):\n","    \"\"\"\n","    Load the triplets from the file and generate the features and labels.\n","\n","    input: file: string, the path to the file containing the triplets\n","          train: boolean, whether the data is for training or testing\n","\n","    output: X: numpy array, the features\n","            y: numpy array, the labels\n","    \"\"\"\n","    triplets = []\n","    with open(file) as f:\n","        for line in f:\n","            triplets.append(line)\n","\n","    # generate training data from triplets\n","    train_dataset = datasets.ImageFolder(root=\"dataset/\",\n","                                         transform=None)\n","    filenames = [s[0].split('/')[-1].replace('.jpg', '') for s in train_dataset.samples]\n","    embeddings = np.load('dataset/embeddings.npy')\n","    # TODO: Normalize the embeddings across the dataset\n","    file_to_embedding = {}\n","    for i in range(len(filenames)):\n","        file_to_embedding[filenames[i]] = embeddings[i]\n","    X = []\n","    y = []\n","    # use the individual embeddings to generate the features and labels for triplets\n","    for t in triplets:\n","        emb = [file_to_embedding[a] for a in t.split()]\n","        X.append(np.hstack([emb[0], emb[1], emb[2]]))\n","        y.append(1)\n","        # Generating negative samples (data augmentation)\n","        if train:\n","            X.append(np.hstack([emb[0], emb[2], emb[1]]))\n","            y.append(0)\n","    X = np.vstack(X)\n","    y = np.hstack(y)\n","\n","    global input_scaler\n","    input_scaler = preprocessing.StandardScaler().fit(X)\n","    X_scaled = input_scaler.transform(X)\n","    return X_scaled, y\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Create data loaders for the training and testing data"]},{"cell_type":"code","execution_count":189,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.602048Z","iopub.status.busy":"2023-05-09T20:21:58.601615Z","iopub.status.idle":"2023-05-09T20:21:58.616521Z","shell.execute_reply":"2023-05-09T20:21:58.615013Z","shell.execute_reply.started":"2023-05-09T20:21:58.602014Z"},"trusted":true},"outputs":[],"source":["# Hint: adjust batch_size and num_workers to your PC configuration, so that you don't run out of memory\n","def create_loader_from_np(X, y = None, train = True, batch_size=64, shuffle=True, num_workers = 4):\n","    \"\"\"\n","    Create a torch.utils.data.DataLoader object from numpy arrays containing the data.\n","\n","    input: X: numpy array, the features\n","           y: numpy array, the labels\n","    \n","    output: loader: torch.data.util.DataLoader, the object containing the data\n","    \"\"\"\n","    if train:\n","        dataset = TensorDataset(torch.from_numpy(X).type(torch.float), \n","                                torch.from_numpy(y).type(torch.long))\n","    else:\n","        dataset = TensorDataset(torch.from_numpy(X).type(torch.float))\n","    loader = DataLoader(dataset=dataset,\n","                        batch_size=batch_size,\n","                        shuffle=shuffle,\n","                        pin_memory=True, num_workers=num_workers)\n","    return loader"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Define a model and train it"]},{"cell_type":"code","execution_count":190,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.618544Z","iopub.status.busy":"2023-05-09T20:21:58.618218Z","iopub.status.idle":"2023-05-09T20:21:58.634711Z","shell.execute_reply":"2023-05-09T20:21:58.633637Z","shell.execute_reply.started":"2023-05-09T20:21:58.618516Z"},"trusted":true},"outputs":[],"source":["# TODO: define a model. Here, the basic structure is defined, but you need to fill in the details\n","class Net(nn.Module):\n","    \"\"\"\n","    The model class, which defines our classifier.\n","    \"\"\"\n","    def __init__(self):\n","        \"\"\"\n","        The constructor of the model.\n","        \"\"\"\n","        super().__init__()\n","        # self.fc = nn.Linear(3000, 1)\n","\n","        self.hidden = nn.Linear(embedding_size_global*3, 20)\n","        self.relu = nn.ReLU()\n","        self.bn1 = nn.BatchNorm1d(20)\n","        self.hidden1 = nn.Linear(20, 20)\n","        self.relu1 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm1d(20)\n","        self.hidden2 = nn.Linear(20, 20)\n","        self.relu2 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm1d(20)\n","        self.out = nn.Linear(20, 1) \n","        self.sigmoid = nn.Sigmoid()\n","        \n","\n","    def forward(self, x):\n","        \"\"\"\n","        The forward pass of the model.\n","\n","        input: x: torch.Tensor, the input to the model\n","\n","        output: x: torch.Tensor, the output of the model\n","        \"\"\"\n","        x = F.relu(self.hidden(x))\n","        x = F.relu(self.hidden1(self.bn1(x)))\n","        x = F.relu(self.hidden2(self.bn2(x)))\n","        x = F.sigmoid(self.out(self.bn3(x)))\n","        return x\n"]},{"cell_type":"code","execution_count":191,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.676242Z","iopub.status.busy":"2023-05-09T20:21:58.675854Z","iopub.status.idle":"2023-05-09T20:21:58.689422Z","shell.execute_reply":"2023-05-09T20:21:58.688197Z","shell.execute_reply.started":"2023-05-09T20:21:58.676208Z"},"trusted":true},"outputs":[],"source":["def train_model(train_loader):\n","    \"\"\"\n","    The training procedure of the model; it accepts the training data, defines the model \n","    and then trains it.\n","\n","    input: train_loader: torch.data.util.DataLoader, the object containing the training data\n","    \n","    output: model: torch.nn.Module, the trained model\n","    \"\"\"\n","    model = Net()\n","    model.train()\n","    model.to(device)\n","    # n_epochs = 10\n","    n_epochs = 10 \n","    # TODO: define a loss function, optimizer and proceed with training. Hint: use the part \n","    # of the training data as a validation split. After each epoch, compute the loss on the \n","    # validation split and print it out. This enables you to see how your model is performing \n","    # on the validation data before submitting the results on the server. After choosing the \n","    # best model, train it on the whole training data.\n","\n","    loss_function = nn.L1Loss() \n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    for epoch in range(n_epochs):        \n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            output = model(data)\n","            loss = loss_function(torch.squeeze(output), target.to(torch.float32))\n","            # loss = loss_function(torch.squeeze(output), target)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if batch_idx % 300 == 0:\n","                print('Epoch {}, Batch idx {}, loss {}'.format(epoch, batch_idx, loss.item()))\n","\n","    print(\"-- finished training --\")\n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Test the model on the test data"]},{"cell_type":"code","execution_count":192,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.691410Z","iopub.status.busy":"2023-05-09T20:21:58.691022Z","iopub.status.idle":"2023-05-09T20:21:58.708849Z","shell.execute_reply":"2023-05-09T20:21:58.707632Z","shell.execute_reply.started":"2023-05-09T20:21:58.691374Z"},"trusted":true},"outputs":[],"source":["def test_model(model, loader):\n","    \"\"\"\n","    The testing procedure of the model; it accepts the testing data and the trained model and \n","    then tests the model on it.\n","\n","    input: model: torch.nn.Module, the trained model\n","           loader: torch.data.util.DataLoader, the object containing the testing data\n","        \n","    output: the predictions\n","    \"\"\"\n","    model.eval()\n","    predictions = []\n","    # Iterate over the test data\n","    with torch.no_grad(): # We don't need to compute gradients for testing\n","        for [x_batch] in loader:\n","            x_batch= x_batch.to(device)\n","            predicted = model(x_batch)\n","            predicted = predicted.cpu().numpy()\n","            # Rounding the predictions to 0 or 1\n","            predicted[predicted >= 0.5] = 1\n","            predicted[predicted < 0.5] = 0\n","            predictions.append(predicted)\n","        predictions = np.vstack(predictions)\n","\n","    return predictions"]},{"cell_type":"code","execution_count":193,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:24:26.934423Z","iopub.status.busy":"2023-05-09T20:24:26.933984Z","iopub.status.idle":"2023-05-09T20:24:26.943658Z","shell.execute_reply":"2023-05-09T20:24:26.941918Z","shell.execute_reply.started":"2023-05-09T20:24:26.934382Z"},"trusted":true},"outputs":[],"source":["def get_accuracy(predictions, target):\n","    '''\n","    IN: output : torch.Tensor([n])\n","        target : torch.Tensor([n])\n","    OUT: (#correct_predictions, n)\n","    '''\n","    output = predictions.clone().detach()\n","    torch.logical_xor(target,predictions,out=output)\n","    torch.logical_not(output,out=output)\n","    return torch.sum(output)/output.size(dim=0)\n","\n","def evaluate_model(model, loader, y):\n","    predictions = test_model(model, loader)\n","    predictions = torch.from_numpy(predictions)\n","\n","    accuracy = get_accuracy(predictions.squeeze(), y)\n","    # l1_loss = np.sum(abs(predictions.squeeze() - y))/len(y)\n","\n","    # print(\"Have a loss of\",l1_loss)\n","    print(\"Accuracy on validation set\",accuracy)"]},{"cell_type":"code","execution_count":194,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.730701Z","iopub.status.busy":"2023-05-09T20:21:58.730351Z","iopub.status.idle":"2023-05-09T20:21:58.744188Z","shell.execute_reply":"2023-05-09T20:21:58.742715Z","shell.execute_reply.started":"2023-05-09T20:21:58.730672Z"},"trusted":true},"outputs":[],"source":["def test_and_save(model, loader):\n","    \"\"\"\n","    runs test model and saves the predictions\n","    \"\"\"\n","\n","    predictions = test_model(model, loader)\n","    np.savetxt(\"results.txt\", predictions, fmt='%i')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Main"]},{"cell_type":"code","execution_count":195,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.747766Z","iopub.status.busy":"2023-05-09T20:21:58.746453Z","iopub.status.idle":"2023-05-09T20:21:58.758324Z","shell.execute_reply":"2023-05-09T20:21:58.757101Z","shell.execute_reply.started":"2023-05-09T20:21:58.747708Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-- main function running --\n"]}],"source":["print(\"-- main function running --\")\n","\n","TRAIN_TRIPLETS = 'train_triplets.txt'\n","TEST_TRIPLETS = 'test_triplets.txt'"]},{"cell_type":"code","execution_count":196,"metadata":{},"outputs":[],"source":["\n","# generate embedding for each image in the dataset\n","if(os.path.exists('dataset/embeddings.npy') == False):\n","    generate_embeddings()"]},{"cell_type":"code","execution_count":197,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:21:58.765345Z","iopub.status.busy":"2023-05-09T20:21:58.764413Z","iopub.status.idle":"2023-05-09T20:22:46.004799Z","shell.execute_reply":"2023-05-09T20:22:46.003555Z","shell.execute_reply.started":"2023-05-09T20:21:58.765295Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-- loaded the data --\n"]}],"source":["# load the training and testing data\n","X, y = get_data(TRAIN_TRIPLETS)\n","X_test, _ = get_data(TEST_TRIPLETS, train=False)\n","\n","print(\"-- loaded the data --\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Local testing"]},{"cell_type":"code","execution_count":198,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:22:46.007286Z","iopub.status.busy":"2023-05-09T20:22:46.006187Z","iopub.status.idle":"2023-05-09T20:22:46.579318Z","shell.execute_reply":"2023-05-09T20:22:46.578234Z","shell.execute_reply.started":"2023-05-09T20:22:46.007248Z"},"trusted":true},"outputs":[],"source":["# p = 0.8\n","# length = y.shape[0]\n","# train_loader = create_loader_from_np(X[:int(length*p)], y[:int(length*p)], train = True, batch_size=64)\n","# test_loader = create_loader_from_np(X[int(length*p):], train = False, batch_size=2048, shuffle=False)"]},{"cell_type":"code","execution_count":199,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:22:46.581847Z","iopub.status.busy":"2023-05-09T20:22:46.580970Z","iopub.status.idle":"2023-05-09T20:23:47.183864Z","shell.execute_reply":"2023-05-09T20:23:47.182506Z","shell.execute_reply.started":"2023-05-09T20:22:46.581802Z"},"trusted":true},"outputs":[],"source":["# model = train_model(train_loader)\n","# print(\"-- trained model --\")"]},{"cell_type":"code","execution_count":200,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:24:31.320416Z","iopub.status.busy":"2023-05-09T20:24:31.320017Z","iopub.status.idle":"2023-05-09T20:24:32.487190Z","shell.execute_reply":"2023-05-09T20:24:32.485407Z","shell.execute_reply.started":"2023-05-09T20:24:31.320385Z"},"trusted":true},"outputs":[],"source":["# evaluate_model(model, test_loader, torch.from_numpy(y[int(length*p):]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Not local testing"]},{"cell_type":"code","execution_count":201,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:24:54.366492Z","iopub.status.busy":"2023-05-09T20:24:54.366049Z","iopub.status.idle":"2023-05-09T20:24:56.550712Z","shell.execute_reply":"2023-05-09T20:24:56.549700Z","shell.execute_reply.started":"2023-05-09T20:24:54.366446Z"},"trusted":true},"outputs":[],"source":["# Create data loaders for the training and testing data\n","train_loader = create_loader_from_np(X, y, train = True, batch_size=64)\n","test_loader = create_loader_from_np(X_test, train = False, batch_size=2048, shuffle=False)"]},{"cell_type":"code","execution_count":202,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:25:00.720632Z","iopub.status.busy":"2023-05-09T20:25:00.719737Z","iopub.status.idle":"2023-05-09T20:26:19.504471Z","shell.execute_reply":"2023-05-09T20:26:19.502704Z","shell.execute_reply.started":"2023-05-09T20:25:00.720592Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Batch idx 0, loss 0.7540867924690247\n","Epoch 0, Batch idx 300, loss 0.6620106101036072\n","Epoch 0, Batch idx 600, loss 0.5760449171066284\n","Epoch 0, Batch idx 900, loss 0.5291057825088501\n","Epoch 0, Batch idx 1200, loss 0.544846773147583\n","Epoch 0, Batch idx 1500, loss 0.5723381042480469\n","Epoch 0, Batch idx 1800, loss 0.5197571516036987\n","Epoch 1, Batch idx 0, loss 0.46931344270706177\n","Epoch 1, Batch idx 300, loss 0.39366286993026733\n","Epoch 1, Batch idx 600, loss 0.49956613779067993\n","Epoch 1, Batch idx 900, loss 0.5665088891983032\n","Epoch 1, Batch idx 1200, loss 0.45510056614875793\n","Epoch 1, Batch idx 1500, loss 0.4190484285354614\n","Epoch 1, Batch idx 1800, loss 0.5262755751609802\n","Epoch 2, Batch idx 0, loss 0.3991261124610901\n","Epoch 2, Batch idx 300, loss 0.2637219727039337\n","Epoch 2, Batch idx 600, loss 0.44398990273475647\n","Epoch 2, Batch idx 900, loss 0.4657508134841919\n","Epoch 2, Batch idx 1200, loss 0.3621496260166168\n","Epoch 2, Batch idx 1500, loss 0.3967718780040741\n","Epoch 2, Batch idx 1800, loss 0.3810712993144989\n","Epoch 3, Batch idx 0, loss 0.3394060730934143\n","Epoch 3, Batch idx 300, loss 0.36008477210998535\n","Epoch 3, Batch idx 600, loss 0.3135528862476349\n","Epoch 3, Batch idx 900, loss 0.36965233087539673\n","Epoch 3, Batch idx 1200, loss 0.31825077533721924\n","Epoch 3, Batch idx 1500, loss 0.33572232723236084\n","Epoch 3, Batch idx 1800, loss 0.32185766100883484\n","Epoch 4, Batch idx 0, loss 0.32974401116371155\n","Epoch 4, Batch idx 300, loss 0.2856677174568176\n","Epoch 4, Batch idx 600, loss 0.4400697946548462\n","Epoch 4, Batch idx 900, loss 0.26416563987731934\n","Epoch 4, Batch idx 1200, loss 0.20426321029663086\n","Epoch 4, Batch idx 1500, loss 0.3065129518508911\n","Epoch 4, Batch idx 1800, loss 0.3250270485877991\n","Epoch 5, Batch idx 0, loss 0.24465949833393097\n","Epoch 5, Batch idx 300, loss 0.2365526705980301\n","Epoch 5, Batch idx 600, loss 0.1559661328792572\n","Epoch 5, Batch idx 900, loss 0.4624369442462921\n","Epoch 5, Batch idx 1200, loss 0.24524995684623718\n","Epoch 5, Batch idx 1500, loss 0.29164090752601624\n","Epoch 5, Batch idx 1800, loss 0.20852649211883545\n","Epoch 6, Batch idx 0, loss 0.23311637341976166\n","Epoch 6, Batch idx 300, loss 0.1127970963716507\n","Epoch 6, Batch idx 600, loss 0.14244988560676575\n","Epoch 6, Batch idx 900, loss 0.4099942445755005\n","Epoch 6, Batch idx 1200, loss 0.14154250919818878\n","Epoch 6, Batch idx 1500, loss 0.14699265360832214\n","Epoch 6, Batch idx 1800, loss 0.1760859191417694\n","Epoch 7, Batch idx 0, loss 0.1756686568260193\n","Epoch 7, Batch idx 300, loss 0.13381893932819366\n","Epoch 7, Batch idx 600, loss 0.21918490529060364\n","Epoch 7, Batch idx 900, loss 0.15926097333431244\n","Epoch 7, Batch idx 1200, loss 0.23607811331748962\n","Epoch 7, Batch idx 1500, loss 0.1384776383638382\n","Epoch 7, Batch idx 1800, loss 0.16725558042526245\n","Epoch 8, Batch idx 0, loss 0.2842051386833191\n","Epoch 8, Batch idx 300, loss 0.12349431216716766\n","Epoch 8, Batch idx 600, loss 0.20027919113636017\n","Epoch 8, Batch idx 900, loss 0.27308616042137146\n","Epoch 8, Batch idx 1200, loss 0.14120838046073914\n","Epoch 8, Batch idx 1500, loss 0.20741000771522522\n","Epoch 8, Batch idx 1800, loss 0.07613760977983475\n","Epoch 9, Batch idx 0, loss 0.13869385421276093\n","Epoch 9, Batch idx 300, loss 0.12874066829681396\n","Epoch 9, Batch idx 600, loss 0.12753430008888245\n","Epoch 9, Batch idx 900, loss 0.3757181465625763\n","Epoch 9, Batch idx 1200, loss 0.061413202434778214\n","Epoch 9, Batch idx 1500, loss 0.2555576264858246\n","Epoch 9, Batch idx 1800, loss 0.21311181783676147\n","-- finished training --\n","-- trained model --\n"]}],"source":["# define a model and train it\n","model = train_model(train_loader)\n","print(\"-- trained model --\")"]},{"cell_type":"code","execution_count":203,"metadata":{"execution":{"iopub.execute_input":"2023-05-09T20:26:35.994569Z","iopub.status.busy":"2023-05-09T20:26:35.994116Z","iopub.status.idle":"2023-05-09T20:26:38.091802Z","shell.execute_reply":"2023-05-09T20:26:38.090121Z","shell.execute_reply.started":"2023-05-09T20:26:35.994531Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Results saved to results.txt\n"]}],"source":["# test the model on the test data\n","test_and_save(model, test_loader)\n","print(\"Results saved to results.txt\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
